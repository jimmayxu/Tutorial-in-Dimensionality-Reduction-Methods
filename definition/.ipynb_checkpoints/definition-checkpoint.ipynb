{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "$('#maintoolbar-container').children('#toggleButton').remove()\n",
       "\n",
       "var toggle_button = (\"<button id='toggleButton' type='button'>Show Code</button>\");\n",
       "$('#maintoolbar-container').append(toggle_button);\n",
       "\n",
       "var code_shown = false;\n",
       "\n",
       "function code_toggle()\n",
       "{\n",
       "\n",
       "    if (code_shown)\n",
       "    {\n",
       "        console.log(\"code shown\")\n",
       "        $('div.input').hide('500');\n",
       "        $('#toggleButton').text('Show Code');\n",
       "    }\n",
       "    else\n",
       "    {\n",
       "        console.log(\"code not shown\")\n",
       "        $('div.input').show('500');\n",
       "        $('#toggleButton').text('Hide Code');\n",
       "    }\n",
       "\n",
       "    code_shown = !code_shown;\n",
       "}\n",
       "\n",
       "$(document).ready(function()\n",
       "{\n",
       "    code_shown=false;\n",
       "    $('div.input').hide();\n",
       "});\n",
       "\n",
       "$('#toggleButton').on('click', code_toggle);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../button.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "<center><h1>Definition of Dimensionality Reduction Methods</h1></center>\n",
    "------\n",
    "\n",
    "Here is the formal definition of each of the methods, these will help us to have a better understanding of the algorithm behind the graphs, and these will be intuitive for the reasons of characteristics.\n",
    "\n",
    " Characteristics | Variance Contribution | Pairwise distance | Neighborhood selection| \n",
    "  ------------- | ------------- | ------------- | ------------- |\n",
    " [Linear Principal Component Analysis](#pca)|  $\\times$ (lower dimension projection) |   |   |\n",
    " [Multidimensional Scaling](#mds)|   |  $\\times$ (one-to-one) |   |\n",
    " [Local Linear Embedding](#LLE)|   |   |$\\times$ (one-to-neighbor under Euclidean distance)   |\n",
    " [Isomap](#isomap)|   |   |   $\\times$(one-to-neighbor under Geodesic distance) |\n",
    " [Kernel Principal Component Analysis](#kernel pca)| $\\times$ (higher dimension projection)   |   |   |\n",
    " [t-distributed Stochastic Neighbor Embedding](#tsne)|   | $\\times$(one-to-all)  |    |\n",
    "\n",
    "[Summary](#summary)\n",
    "\n",
    "[[back to outline](../Tutorial notebook.ipynb#top)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Nomenclature\n",
    "\\begin{align}\n",
    "i,j & \\qquad \\text{indices of data points}\\\\\n",
    "K&  \\qquad \\text{number of nesrest neighbor}\\\\\n",
    "n&  \\qquad \\text{number of data points}\\\\\n",
    "D& \\qquad \\text{dimension of original space}\\\\\n",
    "M& \\qquad \\text{dimension of output space}\\\\\n",
    "d& \\qquad \\text{dimension of embedded space}\\\\\n",
    "X& \\qquad \\text{input data matrix}\\\\\n",
    "X_j& \\qquad \\text{the $j$ columns of matrix $X$}\\\\\n",
    "x_i&  \\qquad \\text{data point in the input space}\\\\\n",
    "y_i&  \\qquad\\text{representation of point $i$ in output space}\\\\\n",
    "\\Phi() & \\qquad \\text{feature function}\\\\\n",
    "d(x_i,x_j)& \\qquad \\text{distance in the input space}\\\\\n",
    "d(y_i,y_j)&  \\qquad\\text{distance in the output space}\\\\\n",
    "\\mathbf{\\lambda}&  \\qquad\\text{vector of eigenvalues}\\\\\n",
    "p_{ij},q_{ij}&\\qquad  \\text{probability of $i$ being a neighbor of $j$}\\\\\n",
    "\\vec{*}[k] &\\qquad \\text{the $k$-th entry of vector $\\vec{*}$}\\\\\n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pca'></a>\n",
    "\n",
    "# 1. Linear Principal Component Analysis\n",
    "\n",
    "[[back to top](#top)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Principal component analysis (PCA) and Multidimensional scaling (MDS), are simple to implement, efficiently computable, and aim to discover informative interpretation from data lying on or near a linear subspace of high-dimensional input space. The goal of principal component analysis (PCA) is to find a lower dimensional embedding of the data points that maximally preserve the variance as measured in the high-dimensional input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- Jolliffe, Ian T. [\"Principal Component Analysis and Factor Analysis.\"](https://rd.springer.com/chapter/10.1007/978-1-4757-1904-8_7) Principal component analysis. Springer New York, 1986. 115-128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mds'></a>\n",
    "\n",
    "# 2. Multidimensional Scaling\n",
    "\n",
    "[[back to top](#top)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "There are several different variants of multidimensional scaling (MDS), but their common goal is to find a configuration of points that preserves the **interpoint distances**, equivalent to PCA when those distances are Euclidean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear MDS\n",
    "\n",
    "sad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric MDS\n",
    "A slightly more complex version is metric MDS. Its cost function is \n",
    "$$E = \\sum_{ij}\\left(d(x_i,x_j) - d(y_i-y_j)\\right)^2,$$\n",
    "where $d$ is a similarity function such as Euclidean distance, $d(x_i,x_j)$ is the distance in the input space and $d(y_i,y_j)$ the distance in the output space, between the representations $y_i$ and $y_j$ of the points $i$ and $j$. The cost function is minimized with respect to the representations $y_i$. Most version of MDS use a variant of this cost function.\n",
    "\n",
    " In non-metric MDS, the distances $d$ are modified by a monotonic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation to linear PCA\n",
    "Gower (1996) has shown that when the dimensionality of the solutions is the same, the projection of the original data to the PCA subspace equals the configuration of points found by linear MDS that is calculated from the Euclidean distance matrix of the data. Thus the cost function of PCA tries to preserve the squared distances between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Venna, Jarkko, and Samuel Kaski. [\"Local multidimensional scaling.\"](http://www.sciencedirect.com/science/article/pii/S0893608006000724) Neural Networks 19.6 (2006): 889-899. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lle'></a>\n",
    "\n",
    "# 3. Locally Linear Embedding \n",
    "\n",
    "[[back to top](#top)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "> Locally linear embedding (LLE) is an unsupervised learning algorithm that computes low-dimensional, neighbourhood-preserving embeddings of high-dimensional inputs. It will preserve the neighbours of each high dimensional data point by assigning weights to the certain amount of local neighbours called *reconstruction weight*, $W_{ij}$, the weight of $x_j$ being a neighbour of $x_i$. As a result, LLE is able to learn the global structure of nonlinear manifolds (data type), such as those generated by images of faces or documents of text.\n",
    "\n",
    "> LLE is suitable to be implemented on [manifold data](../casestudy/casestudy.ipynb#manifold). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    " <img src=\"lle2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    " \n",
    " The figure above illustrates three steps of the algorithm. Suppose the data consists of $N$ real-valued vectors $\\vec{X}_i$, each dimensionality $D$, sampled from some underlying manifold.\n",
    " \n",
    " 1. **Select neighbours** - There are several criterions to select neighbours for each data point. For example, *$K$-nearest neighbour* is a popular method measured by Euclidean distance or normalized dot product, $K$ is the only parameter determined by the user to set the number of nearest neighbours needed to be considered. Notice that the neighbourhood relation is unsymmetric, which means $i$ might not be the neighbour of $j$, given $j$ is a neighbour of $i$.\n",
    " \n",
    " 2. **Reconstruct with linear weights** - Provided there is sufficient data, we expect each data point and its neighbours to lie on or close to a locally linear patch of the manifold, and reconstruction weights $W$ are able to characterize the local geometry of these patches by reconstructing each data point from its neighbour. We compute $W$ by minimising reconstruction errors subject to the constraints that $W_{ij} = 0$ if $i$ and $j$ are not neighbors, and $\\sum_j W_{ij} = 1$.\n",
    " \n",
    " $$\\epsilon (W) = \\sum_i \\left|\\vec{X}_i - \\sum_j W_{ij}\\vec{X}_j\\right|^2$$\n",
    " where the weights $W_{ij}$ summarize the contribution of the $j$th data point to the $i$th reconstruction. By design, the reconstruction weights reflect intrinsic geometric properties of the data that are invariant to linear mapping - translation, rotation and rescaling.\n",
    " \n",
    " 3. **Map to embedded coordinates** - For visualisation, we want to reduce the dimensionality of the data to two or three. Each high-dimensional observation $\\vec{X}_i$ is mapped to a low-dimensional vector $\\vec{Y}_i$ representing global internal coordinates on the manifold. ${\\vec{Y}_i}$ is chosen by minimising the embedding cost function\n",
    " \n",
    " $$\\Phi (Y) = \\sum_i \\left|\\vec{Y}_i - \\sum_j W_{ij}\\vec{Y}_j\\right|^2.$$\n",
    " Subject to constraints of $W$, it can be minimised by solving a soarse $N \\times N$ eigenvalue problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "> **Drug data:** //one application could be on the high-resolution image co-registration:  MALDI optical images of, say, the several slices of mice's brain are combined by *rigid/deformable registration*, but I think that LLE can also be used as an alternative due to special features of data set.//\n",
    "\n",
    "> **Face recognition:** the mapped data points representing photos with continuous change of facial expression and pose of a person will be close to each other and form a string. The figure below shows the part of the embedding space described by the first two coordinates of LLE.\n",
    "\n",
    "> <img src=\"lle3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "> Figure below corresponds to points along the top-right path of figure above (linked by solid line),\n",
    "illustrating one particular mode of variability in pose and expression.\n",
    "\n",
    "> <img src=\"lle4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension\n",
    "\n",
    "One well-known issue with LLE is the regularization problem. When the number of neighbours is greater than the number of input dimensions\n",
    "\n",
    "$$K > D,$$\n",
    "\n",
    "the matrix defining each local neighbourhood is rank-deficient. There is another way called **Modified Local Linear Embedding** to address such issue. Besides it, there are many another variant of LLE such as LTSA, Hessian LLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- Roweis, Sam T., and Lawrence K. Saul. [\"Nonlinear dimensionality reduction by locally linear embedding.\"](http://science.sciencemag.org/content/290/5500/2323) science 290.5500 (2000): 2323-2326. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='isomap'></a>\n",
    "\n",
    "# 4. Isomap\n",
    "\n",
    "[[back to top](#top)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    ">The isomap is a variant of linear MDS. It finds a configuration of points that match the given distance matrix.\n",
    "The difference from traditional MDS is in how the distances are defined. Instead of direct pairwise distance $d$ in MDS, isomap uses *geodesic distances*; instead of optimizing the cost function to obtain weights $W$, these are set to the Euclidean distances between the connected points.\n",
    "\n",
    ">The **geodesic distances** are approximated with the shortest path distance calculated along the $K$-nearest-neighbour graph. $K$- the nearest-neighbour graph is a graph in the content of graphical modelling, each data point being a vertex will build a undirected edge to another vertex, as long as one is a neighbour of another. Such setting guarantees that the neighbourhood relation here is symmetric.\n",
    "\n",
    "\n",
    "> <img src=\"isomap1.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "> Figure **(A)** and **(B)** show the 'Swiss roll' data set, their Euclidean distance (dashed line) in the high-dimensional input space for two circled arbitrary points may not accurately reflect their intrinsic similarities, compared with distance measured by geodesic distance (red segments) in **(B)**. However, some cases, like **(C)**, allow an approximation (blue) to the true geodesic path to be computed efficiently rather than do the corresponding graph paths (red)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "**Step 1. Construct neighborhood graph $\\mathcal{G}.$** \n",
    "Define the graph $\\mathcal{G} \\;\\left(V ,E \\right)$ over all data points by \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Observation $x_i$ in data set} \\Rightarrow i \\in V\\\\\n",
    "\\left.\n",
    "\\begin{aligned}\n",
    "    \\mathbf{[\\epsilon-\\text{Isomap}]} \\quad&\\text{if } d_X(i,j)<\\epsilon \\quad\\\\ \n",
    "    \\mathbf{[K-\\text{Isomap}]} \\quad&\\text{if $i$ is one of the $K$ nearest neighbors of $j$}\n",
    "\\end{aligned}\n",
    "\\right\\}\n",
    "\\Rightarrow(i,j)\\in E \n",
    "\\end{equation}\n",
    "The distances $d_X(i,j)$ between all pairs of node $i, j$ from $N$ data points in the high-dimensional input space $X$, measured either in the standard Euclidean metric or in some domain-specific metric.\n",
    "\n",
    "**Step 2. Compute shortest geodesic distance matrix.** Initialize $d_G(i,j)$, and then construct distance matrix $D_G\\in \\mathbb{R}^{n\\times n}$.\n",
    "\n",
    "\\begin{align}\n",
    "  d_G(i,j)&=\\begin{cases}\n",
    "    d_X(i,j), & \\text{if $(i.j)\\in E$}.\\\\\n",
    "    \\infty, & \\text{otherwise}.\n",
    "  \\end{cases}\\\\\n",
    "D_G[i,j] &= \\min_{k\\in[n]} \\left\\{d_G(i,j), \\;d_G(i,k) + d_G(k,j)\\right\\} \\qquad \\forall i\\in[n]\n",
    "\\end{align}\n",
    "Under such construction, entries of $D_G$ will containthe shortest path geodesic distances between all pairs of points in $\\mathcal{G}$.\n",
    "\n",
    "**Step 3. Construct $d$-dimensional embedding.** Let $\\lambda_p$ be the $p$-th eigenvalue (in descending order) of the transformed matrix $\\tau(D_G)$, and $v_p[i]$ be the $i$-th component of the $p$-th eigenvector. Then in the embedded $d$-dimensional space, set $i$-th coordinate vector $y_i$\n",
    "\n",
    "$$y_i [p]= \\sqrt{\\lambda_pv_p [i]}  \\qquad \\forall i,p \\in [d]$$\n",
    "\n",
    "where the operator $\\tau$ is defined by $\\tau(D) = -\\frac{HSH}{2}$, $S$ is the matrix of squared distances $\\left\\{S_{ij} = D_{ij}^2\\right\\}$, and $H$ is the 'centering matrix'$\\left\\{H_{ij} = \\delta_{ij} - 1/N\\right\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "> **Manifold dataset.** Isomap is able to perform better than LLE due to the involvement of geodesic distance in optimization step.\n",
    "\n",
    "> **Interpolations along straight lines in the isomap coordiante space.** \n",
    "**(A)** Interpolations in a three-dimensional embedding of face images.\n",
    "**(B)** Interpolations in a fourdimensional embedding of hand images \n",
    "appear as natural hand movements when viewed in quick succession, even though no\n",
    "such motions occurred in the observed data. \n",
    "**(C)** Interpolations in a six-dimensional embedding of\n",
    "handwritten '2's preserve continuity not\n",
    "only in the visual features of loop and arch articulation,\n",
    "but also in the implied pen trajectories,\n",
    "which are the true degrees of freedom underlying\n",
    "those appearances.\n",
    "\n",
    "> <img src=\"isomap2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Tenenbaum, Joshua B., Vin De Silva, and John C. Langford. [\"A global geometric framework for nonlinear dimensionality reduction.\"](http://science.sciencemag.org/content/290/5500/2319) science 290.5500 (2000): 2319-2323."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel pca'></a>\n",
    "\n",
    "# 5. Kernel principal component analysis\n",
    "\n",
    "[[back to top](#top)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"kernel1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation function $\\phi$ from the $D$-dimensional data set $\\{x_i\\}_{i\\in[n]}$  to an $n$-dimensional space.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Feature function}\\quad&\\Phi:x_i \\in \\mathbb{R}^D\\longmapsto u_i\\\\ \n",
    "\\text{Linear transformation}\\quad& \\phi: u_i \\longmapsto y_i\\in\\mathbb{R}^n\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dimension of projected space $M = n \\gg D$. Here is the detail of algorithms\n",
    "\n",
    "$$y_i[k] = \\phi(\\Phi(x))[k] = V^k\\Phi(x) = \\sum_{i}\\left(a_i^k \\Phi(x_i)\\cdot\\Phi(x)\\right)$$\n",
    "\n",
    "where $\\kappa(x_i,x_j) = \\Phi(x_i)^T\\Phi(x_j)$ is an evaluation of a kernel $\\kappa(u,v)$ at $(u,v) = (x_i,x_j)$ for all $i,j \\in [n]$, here we introduce two popular kernel\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Ploynomial kernel:}& \\quad\\kappa(u,v) = (u^Tv + c)^d\\\\\n",
    "\\text{Gaussian kernel:}&\\quad \\kappa(u,v) = \\exp\\left(-\\frac{||u-v||^2}{2\\sigma^2}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter\n",
    "The hyperparameter for Gaussian kernel PCA is called $\\gamma$, where\n",
    "\n",
    "$$\\gamma = \\frac{1}{2\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Schölkopf, Bernhard, Alexander Smola, and Klaus-Robert Müller. [\"Kernel principal component analysis.\"](https://rd.springer.com/chapter/10.1007/BFb0020217) International Conference on Artificial Neural Networks. Springer, Berlin, Heidelberg, 1997."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tsne'></a>\n",
    "\n",
    "# 6. t-distributed Stochastic Neighbour Embedding\n",
    "\n",
    "[[back to top](#top)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t-distributed stochastic neighbour embedding non-linear dimensionality reduction algorithm suitable for visualization. It is based on probability distributions with random walk on neighbourhood graphs to find the structure within the data. It is a methods more advanced than PCA, one of the important feature is that similar data points are represented close together in the projected space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "Manifold learning is an approach to nonlinear dimensionality reduction.\n",
    "  1.  It uses a student t distribution to compute the similarity between two points in the low-dimensional space.\n",
    "  \n",
    "  2.  **Type of stochastic neighbour embedding (SNE)** determined by the distribution of conditional probability.\n",
    "      - **Gaussian distributed SNE** \n",
    "      \\begin{equation}\n",
    "      p_{j|i} \\sim N\\left(x_i,σ_i^2\\right), \\quad q_{j|i} \\sim N\\left(y_i,\\frac{1}{2}\\right)\n",
    "      \\end{equation}\n",
    "      Known conditional probability on original space with data $\\{x_i\\}$:\n",
    "       \\begin{equation}\n",
    "      p_{j|i} = \\frac{\\exp\\left(-||x_i-x_j||^2\\right) /\\, 2\\sigma_i^2}{\\sum_{k\\neq i}\\exp\\left(-||x_i-x_k||^2\\right) /\\, 2\\sigma_i^2}\n",
    "      \\end{equation}\n",
    "      Unknown conditional probability on projected space with data $\\{y_i\\}$:\n",
    "       \\begin{equation}\n",
    "      q_{j|i} = \\frac{\\exp\\left(-||y_i-y_j||^2\\right)}{\\sum_{k\\neq i}\\exp\\left(-||x_i-x_k||^2\\right)}\n",
    "      \\end{equation}\n",
    "      \n",
    "      - **Student t distributed SNE**\n",
    "      \\begin{equation}\n",
    "      p_{ij}\\,=\\,p_{ji}\\,=\\, \\frac{p_{j|i}+p_{i|j}}{2n},\\quad q_{ij}\\,=\\,q_{ji}\\,=\\, \\frac{q_{j|i}+q_{i|j}}{2n}\n",
    "      \\end{equation}\n",
    "      \n",
    "  3. Optimisation of cost function with respect to projected coordinates $\\{y_k\\}_{k\\in[n]}$\n",
    "      - **Asymmetric Gaussian SNE ** It uses conditional probability $p_{j|i}$ and $q_{j|i}$ to minimise the sum of the Kullback-Leibler divergences \n",
    "      \n",
    "      \\begin{equation}\n",
    "      C = \\sum_{i}KL(P_i||Q_i) = \\sum_i\\sum_jp_{j|i}\\log\\frac{p_{j|i}}{q_{j|i}}\n",
    "      \\end{equation}      \n",
    "      If the projected points $y_i$ and $y_j$ correctly model the similarities between $x_i$ and $x_j$ in original dimensional space, then the conditional probabilities \n",
    "      \n",
    "      $$p_{j|i} = q_{j|i}\\quad \\forall i,j$$\n",
    "      - **Symmetric t distributed SNE ** It minimises the sum of the Kullback-Leibler divergences between joint probability distributions $P$ and $Q$ in original and projected space respectively.\n",
    "      \n",
    "      \\begin{equation}\n",
    "      C = \\sum_{i}KL(P||Q) = \\sum_i\\sum_jp_{ij}\\log\\frac{p_{ij}}{q_{ij}}\n",
    "      \\end{equation}  \n",
    "  4. Minimising the cost function by *gradient descent* methods over unknown projected data points $\\{y_i\\}_{i\\in[n]}$\n",
    "      - **Gaussian-SNE** $\\frac{\\delta C}{\\delta y_i} = 2 \\sum_j \\left(p_{j|i} - q_{j|i} + p_{i|j} - q_{i|j}\\right)\\left(y_i - y_j\\right)$\n",
    "      \n",
    "      - **t-SNE**        $\\frac{\\delta C}{\\delta y_i} = 2 \\sum_j (p_{ij} - q_{ij})(y_i-y_j)$\n",
    "      \n",
    "      The gradient may be interpreted as the resultant force created by a set of springs between the map point $y_i$ and all other map points $y_j$.\n",
    "      \n",
    "      <img src=\"tsne1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>   \n",
    "      \n",
    "  5. Predetermined hyperparameter *perplexity $Perp$* tuned by users, which has relation with the only unknown parameter $\\{\\sigma_i\\}$, the variance of conditional probability $p_{j|i}$.\n",
    "     - Perplexity is defined as \n",
    "    \n",
    "    \\begin{equation}\n",
    "    Perp(P_i) = 2^{H(P_i)}\n",
    "    \\end{equation}\n",
    "    \n",
    "       where $H(P_i) = -\\sum_j p_{j|i}\\log_2p_{i|j}$\n",
    "     - Perplexity can be interpreted as a smooth measure of the effective number of neighbours. It usually be valued between $5$ to $50$. Given $x_i$, higher value of perplexity will result in higher $H(P_i)$, in turn, higher conditional probability $p_{j|i}$ for all $j$. As a result, it implies more variance involved in the random walk beginning from data $x_i$, i.e. local neighbor of $x_i$ will contribute more to $P_i$.\n",
    "     \n",
    "  6. Brief gradient descent procedure for optimizing the t-SNE cost function\n",
    "   \n",
    "      <img src=\"tsne2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "As far as I know, the data type like handwritten digit explained in the CASESTUDY section can be well visualised by t-SNE. Dataset having similar feature to handwritten digit, such as discrete, unit consistent, will be adviced to use t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Maaten, Laurens van der, and Geoffrey Hinton. [\"Visualizing data using t-SNE.\"](http://www.jmlr.org/papers/v9/vandermaaten08a.html) Journal of Machine Learning Research 9.Nov (2008): 2579-2605."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "\n",
    "# Summary\n",
    "\n",
    "[[back to top](#top)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Contribution Methods: PCA\n",
    ">**Constraints:** The PCA was done on the correlation matrix, so in order to make the covariance matrix to be more appropriate, all measurements should be better to made in the same units.\n",
    "\n",
    "> **Ideal Visualisation**: Clusters representing each group, but the feature of group probably is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhood Selection Methods: LLE and Isomap\n",
    "> **Disadvantage：** What we expect from the plot of isomap is several series of strings, but a problem should be considered that how to connect point to construct strings. We need to do further research on the plot to how to connect points and what feature each string represents. So such work will be more complex than doing classification on the plot generated from PCA. \n",
    "\n",
    " >**Advantage:** covariates no need to have consistent unit (like pixel grey scale, coordinates)\n",
    " \n",
    " > **Advice:** Better not to rescale the data in advance, otherwise the rescaled data will alleviate the difference between observations and will include non-similar dataset when doing neighbourhood selection.\n",
    " \n",
    " > **Ideal Visualisation: ** A series of points connecting each other can form a string, which represents a movement by location or reaction by time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise distance Methods: MDS and t-SNE\n",
    "\n",
    "> **Ideal Visualisation**: Clusters representing each group, but the feature of group probably is unknown."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

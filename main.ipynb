{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "<center><h1>Data Science Tutorial:</h1></center>\n",
    "====\n",
    "<center><h1>Introduction on Selecting Dimensionality Reduction Methods for Exploratory Data nalysis</h1></center>\n",
    "-----\n",
    "\n",
    "\\begin{align}\n",
    " Author:&\\quad\\text{ Zhihan Xu}\\\\\n",
    " Email: &\\quad\\text{ zx243@cam.ac.uk}\\\\\n",
    " Supervisor: &\\quad\\text{ Laura Acqualagna}\\\\\n",
    " Company: &\\quad \\text{GlaxoSmithKline}\\\\\n",
    " Time: &\\quad\\text{ July 2017 - September 2017}\\\\\n",
    " \\end{align}\n",
    " * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scientists working with large volumes of high-dimensional data, such as drug discovery data, global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. This tutorial will provide a general exposure on dimensionality reduction methods and give advice about the suitability of methods implementation onto drug discovery data.\n",
    "\n",
    "In the area of drug discovery data analysis, it compromises with (i) omics, (ii) phsiochemical properties of compounds, (iii) in vitro/in vivo experimental results and (iv) paramters extracted from medical images.\n",
    "\n",
    "a well-known dimensionality reduction method called principal component analysis (PCA) is vastly used, but data scientists usually ignore the limitation and assumption of PCA. Imagine the useful information hidden in a bunch of data is stored in a safe, then currently there are no such methods that will be the key to all the safe. Thus, before trying to use the key such as PCA onto the safe, more efficient and pleasing way is to check the type of your safe (dataset), otherwise, the result of methods will normally be disappointing and meaningless. Similarly, it is better to check the step of your analysis procedure and the type of covariates in your data set before taking a close look at this tutorial. \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Decision Tree for Data Analysis: ](https://www.lucidchart.com/invitations/accept/5f9dd676-f032-4916-940a-e0aff10a8ab1) In this tutorial, we only focus on the step of application of dimensionality reduction methods labelled by the black hexagon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Data type |  Covariate examples  | Suitability of using dimensionality reduction methods directly\n",
    "  ------------- | ------------- | ------------- | \n",
    "  **Continuous/Discrete and unit consistent**   |  coordinates/pixel grey scale| YES\n",
    "  **Continuous/Discrete but unit inconsistent**   | weight,height,salary | YES\n",
    "  **Discrete**| age/number of objects| NO\n",
    "  **Categorical**|marital status/level of education/country | NO\n",
    "  **Binary** | sex/smoke or not| NO\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"decision.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Definition  | Data Type*     | Chracteristic | Parameter Tuning| Linearity Related to Input Space| Data Retrievable\n",
    "  ------------- | ------------- | ------------- | ------------- | ------------- | -------------\n",
    "  [**Linear PCA**](definition/definition.ipynb#pca)   |  numerical | variance contribution   | N/A  |  [Linear]() | exact |\n",
    "  [**Multidimensional Scaling**](definition.ipynb#kernel pca)|   numerical | parwise distance  | distance function $d$  |  [Linear]() | exact |  \n",
    "  [**Local Linear Embedding**](definition.ipynb#kernel pca)|   numerical | neighborhood selection   | $K$ neighbors   |  [non-Linear]() | exact |  \n",
    "  [**Isomap **](definition.ipynb#isomap) |   numerical | neighborhood selection   | $K$ neighbors or radius $\\epsilon$   |  [non-Linear]() | approximate |\n",
    "  [**Kernel PCA**](definition.ipynb#kernel pca)|   numerical | variance contribution  | kernel function |  [non-Linear]() | approximate |\n",
    "  [**t-SNE**](definition.ipynb#tsne)|   numerical | pairwise distance   |perplexity $perp$ |  [non-Linear]() | approximate |\n",
    "  \n",
    "  \n",
    "  *The type of input covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: [Definition of Dimensionality Reduction Methods](definition/definition.ipynb)\n",
    "The introduction below includes classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS); nonlinear dimensionality reduction methods such as kernel PCA, which are capable of discovering the nonlinear degrees of freedom that underlie complex natural observations; another algorithm called isomap developed by [Tenenbaum and Langford (2000)](http://science.sciencemag.org/content/290/5500/2319), it computes a globally optimal solution for an important class of data manifolds.\n",
    "\n",
    "\n",
    "1. [Linear Principal Component Analysis](definition/definition.ipynb#pca)\n",
    "- [Multidimensional Scaling](definition/definition.ipynb#mds)\n",
    "- [Local Linear Embedding](definition/definition.ipynb#LLE)\n",
    "- [Isomap](definition/definition.ipynb#isomap)\n",
    "- [Kernel Principal Component Analysis](definition/definition.ipynb#kernel pca)\n",
    "- [t-distributed Stochastic Neighbor Embedding](definition/definition.ipynb#tsne)\n",
    "\n",
    "The methods covered above can be implemented by build-in toolbox developed by [Scikit-Learn](http://scikit-learn.org/stable/unsupervised_learning.html) in the language of Python, while other useful methods such as self-organizing map (SOM) are not introduced here. For readers looking for results of evaluation of methods, a review paper was cited below.\n",
    "\n",
    "A comparison of nonlinear dimensionality reduction methods was made by [Venna and Kaski (2006)](), methods are evaluated by *trustworthiness* of visualisation and *discontinuity* of the mapping. They concluded that isomap and LLE are designed to extract manifolds, while stochastic neighbour embedding (SNE) and self-organizing map (SOM) are more generally targeted for dimensionality reduction. For the purpose of data visualization, SOM and canonical correlation analysis (CCA) were recommended; for purpose of original neighbourhoods preservation, linear PCA and SNE were recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: [Case Study for Various Dataset](casestudy/casestudy.ipynb) \n",
    "#### [function script](casestudy/functions.ipynb)\n",
    "1. [Handwritten digits](casestudy/casestudy.ipynb#digits) \n",
    "- [Concentric circle](casestudy/casestudy.ipynb#concentric circle) \n",
    "- [Mice dataset](casestudy/casestudy.ipynb#mice) \n",
    "- [Eigenfaces](casestudy/casestudy.ipynb#eigenfaces) \n",
    "- [S-shaped Manifolds](casestudy/casestudy.ipynb#manifold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: [Demonstration of Methods](demonstration/demonstration.ipynb)\n",
    "#### [function script](demonstration/functions.ipynb)\n",
    "1. [ t-SNE versus Linear PCA on handwritten dataset](demonstration/demonstration.ipynb#tsne pca)\n",
    "- [ Gaussian Kernel PCA on concentric circle and mice dataset](demonstration/demonstration.ipynb#kernel pca)\n",
    "- [ Factor Analysis versus randomised SVD Linear PCA on eigenfaces dataset](demonstration/demonstration.ipynb#fa pca)\n",
    "- [ Difference of geodesic and Euclidean distance chosen in the methods' algorithm](demonstration/demonstration.ipynb#distance)\n",
    "- [Implementation of pairwise distance methods onto mice data](demonstration/demonstration.ipynb#mice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click to expand\n",
    "\n",
    "<!--bibtex\n",
    "\n",
    "@article{venna2006local,\n",
    "  title={Local multidimensional scaling},\n",
    "  author={Venna, Jarkko and Kaski, Samuel},\n",
    "  journal={Neural Networks},\n",
    "  volume={19},\n",
    "  number={6},\n",
    "  pages={889--899},\n",
    "  year={2006},\n",
    "  publisher={Elsevier}\n",
    "}\n",
    "\n",
    "\n",
    "@article{roweis2000nonlinear,\n",
    "  title={Nonlinear dimensionality reduction by locally linear embedding},\n",
    "  author={Roweis, Sam T and Saul, Lawrence K},\n",
    "  journal={science},\n",
    "  volume={290},\n",
    "  number={5500},\n",
    "  pages={2323--2326},\n",
    "  year={2000},\n",
    "  publisher={American Association for the Advancement of Science}\n",
    "}\n",
    "\n",
    "@article{maaten2008visualizing,\n",
    "  title={Visualizing data using t-SNE},\n",
    "  author={Maaten, Laurens van der and Hinton, Geoffrey},\n",
    "  journal={Journal of Machine Learning Research},\n",
    "  volume={9},\n",
    "  number={Nov},\n",
    "  pages={2579--2605},\n",
    "  year={2008}\n",
    "}\n",
    "\n",
    "@inproceedings{scholkopf1997kernel,\n",
    "  title={Kernel principal component analysis},\n",
    "  author={Sch{\\\"o}lkopf, Bernhard and Smola, Alexander and M{\\\"u}ller, Klaus-Robert},\n",
    "  booktitle={International Conference on Artificial Neural Networks},\n",
    "  pages={583--588},\n",
    "  year={1997},\n",
    "  organization={Springer}\n",
    "}\n",
    "\n",
    "@article{higuera2015self,\n",
    "  title={Self-organizing feature maps identify proteins critical to learning in a mouse model of down syndrome},\n",
    "  author={Higuera, Clara and Gardiner, Katheleen J and Cios, Krzysztof J},\n",
    "  journal={PloS one},\n",
    "  volume={10},\n",
    "  number={6},\n",
    "  pages={e0129126},\n",
    "  year={2015},\n",
    "  publisher={Public Library of Science}\n",
    "}\n",
    "\n",
    "@article{tenenbaum2000global,\n",
    "  title={A global geometric framework for nonlinear dimensionality reduction},\n",
    "  author={Tenenbaum, Joshua B and De Silva, Vin and Langford, John C},\n",
    "  journal={science},\n",
    "  volume={290},\n",
    "  number={5500},\n",
    "  pages={2319--2323},\n",
    "  year={2000},\n",
    "  publisher={American Association for the Advancement of Science}\n",
    "}\n",
    "\n",
    "\n",
    "@article{scikit-learn,\n",
    " title={Scikit-learn: Machine Learning in {P}ython},\n",
    " author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n",
    "         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n",
    "         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n",
    "         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n",
    " journal={Journal of Machine Learning Research},\n",
    " volume={12},\n",
    " pages={2825--2830},\n",
    " year={2011}\n",
    "}\n",
    "\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
